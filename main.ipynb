{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3420a946-fc0a-4a6a-8719-957bc16fa6cc",
   "metadata": {
    "id": "3420a946-fc0a-4a6a-8719-957bc16fa6cc"
   },
   "source": [
    "# Step 0: Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_7sVghyJBMi7",
   "metadata": {
    "id": "_7sVghyJBMi7"
   },
   "outputs": [],
   "source": [
    "# if you need to download uncomment this\n",
    "\n",
    "#pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9755b4b-7e96-4a4b-b317-90f4f99ed679",
   "metadata": {
    "id": "d9755b4b-7e96-4a4b-b317-90f4f99ed679"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import cmudict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6vC7F6zA6bC",
   "metadata": {
    "id": "d6vC7F6zA6bC"
   },
   "outputs": [],
   "source": [
    "# if you need to download uncomment this\n",
    "\n",
    "#nltk.download(\"punkt\")\n",
    "#nltk.download(\"cumdict\")\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576f5502-928e-4dff-b2c5-7ec7c6b5083a",
   "metadata": {
    "id": "576f5502-928e-4dff-b2c5-7ec7c6b5083a"
   },
   "source": [
    "# Step 1: Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6301d277-72f7-49a3-b364-93bb17e39653",
   "metadata": {
    "id": "6301d277-72f7-49a3-b364-93bb17e39653"
   },
   "outputs": [],
   "source": [
    "def data_extraction(link, id):\n",
    "    #getting requests to access\n",
    "    response = requests.get(link)\n",
    "\n",
    "    #checking sucess of request\n",
    "    if response.status_code == 200:\n",
    "        # Parsing through the html doc\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # extracting only article name and content\n",
    "        tag_elements = soup.find_all('div', {'class': 'td-post-content tagdiv-type'})\n",
    "\n",
    "        # condition for checking if tag exists in the html doc\n",
    "        if tag_elements:\n",
    "            # in case tag found extracting text content of article\n",
    "            text_content = \"\\n\".join(tag_element.get_text() for tag_element in tag_elements)\n",
    "\n",
    "            # Saving the article content to a text file with the URL_ID as the filename\n",
    "            with open(f\"{id}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(text_content)\n",
    "            print(f\"Successfully saved content for ID {id}\")\n",
    "        else:\n",
    "          print(f\"Failed to fetch content for ID {id}\")\n",
    "\n",
    "# loading excel file using pandas\n",
    "excel_file = \"Input.xlsx\"\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "# loop to iterete over all URL_IDs\n",
    "for index, row in df.iterrows():\n",
    "    id = row['URL_ID']\n",
    "    link = row['URL']\n",
    "    data_extraction(link, id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f661547e-c9b7-42d9-811d-3795a7f3ea11",
   "metadata": {
    "id": "f661547e-c9b7-42d9-811d-3795a7f3ea11"
   },
   "source": [
    "### Due to the use of different HTML tag in document some links wre not successfully parsed, after checking for required tag, the extraction code is re-executed for missing files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a35b28-60e5-4f8c-8ea9-6cb47b54b680",
   "metadata": {
    "id": "08a35b28-60e5-4f8c-8ea9-6cb47b54b680"
   },
   "outputs": [],
   "source": [
    "def data_extraction(link, id):\n",
    "    #getting requests to access\n",
    "    response = requests.get(link)\n",
    "\n",
    "    #checking sucess of request\n",
    "    if response.status_code == 200:\n",
    "        # Parsing through the html doc\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # extracting only article name and content\n",
    "        tag_elements = soup.find_all('div', {'class': 'tdb-block-inner td-fix-index'})\n",
    "\n",
    "        # condition for checking if tag exists in the html doc\n",
    "        if tag_elements:\n",
    "            # in case tag found extracting text content of article\n",
    "            text_content = \"\\n\".join(tag_element.get_text() for tag_element in tag_elements)\n",
    "\n",
    "            # Saving the article content to a text file with the URL_ID as the filename\n",
    "            with open(f\"{id}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(text_content)\n",
    "            print(f\"Successfully saved content for URL_ID {id}\")\n",
    "        else:\n",
    "          print(f\"Failed to fetch content for URL_ID {id}\")\n",
    "\n",
    "# Load the Excel file\n",
    "#excel_file = \"Input.xlsx\"\n",
    "#df = pd.read_excel(excel_file)\n",
    "\n",
    "# List of missing URL IDs to process\n",
    "specific_ids = ['blackassign0014','blackassign0020','blackassign0029','blackassign0043','blackassign0083','blackassign0084','blackassign0092','blackassign0099','blackassign0100']  # Replace with the IDs you want to process\n",
    "\n",
    "# Iterate through each row in the Excel file\n",
    "for index, row in df.iterrows():\n",
    "    # Extract ID, link, and tag name from the current row\n",
    "    id = row['URL_ID']\n",
    "    link = row['URL']\n",
    "    #tag_name = row['Tag']  # Assuming you have a column named 'Tag' in your Excel file\n",
    "\n",
    "    # Check if the current ID is in the list of specific IDs\n",
    "    if id in specific_ids:\n",
    "        # Scrape website for specific tag and save content\n",
    "        data_extraction(link, id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e651fc-6c35-4d6d-8707-891adb5354a1",
   "metadata": {
    "id": "e7e651fc-6c35-4d6d-8707-891adb5354a1"
   },
   "source": [
    "# Step 2: Removing stopwords from given list of stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ded5c2-9575-45fe-b171-2b46c18fe161",
   "metadata": {
    "id": "99ded5c2-9575-45fe-b171-2b46c18fe161",
    "outputId": "03f9932f-a292-4490-e2d9-72802fefba04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suceeded\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "   # Loading all stopwords files\n",
    "    def load_stopwords_from_files(stopwords_files):\n",
    "        stopwords = set()\n",
    "        for filepath in stopwords_files:\n",
    "            with open(filepath, 'r', encoding='latin-1') as file:\n",
    "                stopwords.update(file.read().splitlines())\n",
    "        return stopwords\n",
    "\n",
    "    # Function to remove stopwords from text\n",
    "    def remove_stopwords(text, stopwords):\n",
    "        return ' '.join([word for word in text.split() if word.lower() not in stopwords])\n",
    "\n",
    "    # List of stopwords files\n",
    "    stopwords_files = ['StopWords/StopWords_Auditor.txt', 'StopWords/StopWords_Currencies.txt', 'StopWords/StopWords_DatesandNumbers.txt','StopWords/StopWords_Generic.txt','StopWords/StopWords_GenericLong.txt','StopWords/StopWords_Geographic.txt','StopWords/StopWords_Names.txt']  # Add more files as needed\n",
    "\n",
    "    # Load stopwords from files\n",
    "    stopwords = load_stopwords_from_files(stopwords_files)\n",
    "\n",
    "    # Iterate through each text file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "\n",
    "            with open(filepath, 'r', encoding='latin-1') as file:\n",
    "                text = file.read()\n",
    "\n",
    "            # Removing stopwords from text\n",
    "            text_without_stopwords = remove_stopwords(text, stopwords)\n",
    "\n",
    "            # Writing text without stopwords\n",
    "            with open(filepath, 'w', encoding='utf-8') as file:\n",
    "                file.write(text_without_stopwords)\n",
    "except Exception as e:\n",
    "    error_message = str(e)\n",
    "    print(\"An error occurred:\", error_message)\n",
    "else:\n",
    "    print(\"Suceeded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17acf74f-50a1-461f-b91c-c7e66e2ee949",
   "metadata": {
    "id": "17acf74f-50a1-461f-b91c-c7e66e2ee949"
   },
   "source": [
    "# Step 3: Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc0ae6a-a1ef-45b6-9330-8caaaafd4678",
   "metadata": {
    "id": "1fc0ae6a-a1ef-45b6-9330-8caaaafd4678"
   },
   "source": [
    "## 1. Calulating:\n",
    "### a)Positive Score\n",
    "### b)Negative Score\n",
    "### c)Polarity Score\n",
    "### d)Subjectivity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d5f29c-0849-4efd-8d34-06785dd3a69e",
   "metadata": {
    "id": "27d5f29c-0849-4efd-8d34-06785dd3a69e",
    "outputId": "5c7afcc5-6b10-4f85-d8fb-6c8f4d00977e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text file 'blackassign0049.txt' not found. Skipping.\n",
      "Succeded\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Function to calculate positivity and negativity scores\n",
    "    def calculate_scores(text, positive_words, negative_words):\n",
    "        positivity_score = sum(1 for word in text.split() if word.lower() in positive_words)\n",
    "        negativity_score = sum(1 for word in text.split() if word.lower() in negative_words)\n",
    "        return positivity_score, negativity_score\n",
    "\n",
    "\n",
    "    # Function to calculate Polarity Score\n",
    "    def calculate_polarity_score(positive_score, negative_score):\n",
    "        polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "        return polarity_score\n",
    "\n",
    "\n",
    "    # Function to calculate Subjectivity Score\n",
    "    def calculate_subjectivity_score(positive_score, negative_score, total_words):\n",
    "        subjectivity_score = (positive_score + negative_score) / (total_words + 0.000001)\n",
    "        return subjectivity_score\n",
    "\n",
    "\n",
    "    # Read positive and negative word lists from Master Dictionary folder\n",
    "    positive_words_file = 'MasterDictionary/positive-words.txt'\n",
    "    negative_words_file = 'MasterDictionary/negative-words.txt'\n",
    "\n",
    "    with open(positive_words_file, 'r', encoding='latin-1') as file:\n",
    "        positive_words = set(file.read().splitlines())\n",
    "\n",
    "    with open(negative_words_file, 'r', encoding='latin-1') as file:\n",
    "        negative_words = set(file.read().splitlines())\n",
    "\n",
    "\n",
    "    # Read existing Excel file provided for output saving\n",
    "    excel_file = 'Output Data Structure.xlsx'\n",
    "    df = pd.read_excel(excel_file)\n",
    "\n",
    "\n",
    "    # Iterate through each URL_ID in the Excel file\n",
    "    for index, row in df.iterrows():\n",
    "        filename = row['URL_ID']+'.txt'\n",
    "        text_filepath = os.path.join('', filename)\n",
    "\n",
    "        try:\n",
    "            # Attempt to open the text file\n",
    "            with open(text_filepath, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "        except FileNotFoundError:\n",
    "            # If the text file does not exist, skip this row\n",
    "            print(f\"Text file '{filename}' not found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Calculating Scores: Positivity, Negativity:\n",
    "        positivity_score, negativity_score = calculate_scores(text, positive_words, negative_words)\n",
    "\n",
    "        # Updating Scores:\n",
    "        df.at[index, 'POSITIVE SCORE'] = positivity_score\n",
    "        df.at[index, 'NEGATIVE SCORE'] = negativity_score\n",
    "\n",
    "        # Calculating Polarity Score:\n",
    "        polarity_score = calculate_polarity_score(positivity_score, negativity_score)\n",
    "\n",
    "        #Updating Polarity Score:\n",
    "        df.at[index, 'POLARITY SCORE'] = polarity_score\n",
    "\n",
    "        # Calculating word count:\n",
    "        total_words = len(text.split())\n",
    "\n",
    "        # Updating word count:\n",
    "        df.at[index, 'WORD COUNT'] = total_words\n",
    "\n",
    "        # Calculate Subjectivity Score\n",
    "        subjectivity_score = calculate_subjectivity_score(positivity_score, negativity_score, total_words)\n",
    "\n",
    "        # Updating Subjectivity Scores:\n",
    "        df.at[index, 'SUBJECTIVITY SCORE'] = subjectivity_score\n",
    "\n",
    "\n",
    "    # Write updated DataFrame back to the Excel file\n",
    "    df.to_excel(excel_file, index=False)\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    error_message = str(e)\n",
    "    print(\"An error occurred:\", error_message)\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Succeded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a302ba-90d6-4f18-83eb-2c9e03dc499d",
   "metadata": {
    "id": "20a302ba-90d6-4f18-83eb-2c9e03dc499d"
   },
   "source": [
    "## 2. Calculating:\n",
    "#### a)AVG SENTENCE LENGTH\n",
    "#### b)PERCENTAGE OF COMPLEX WORDS\n",
    "#### c)SYLLABLE COUNT\n",
    "#### d)AVG WORDS PER SENTENCE\n",
    "#### e)TOTAL COMPLEX WORD COUNT\n",
    "#### f)SYLLABLE PER WORD\n",
    "#### g)PERSONAL PRONOUNS\n",
    "#### h)AVG WORD LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a7a29-d3e7-43fe-ad89-918c2ccff409",
   "metadata": {
    "id": "f95a7a29-d3e7-43fe-ad89-918c2ccff409",
    "outputId": "e6e2bce4-32be-4478-c6b8-f779b156ae82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text file 'blackassign0049.txt' not found. Skipping.\n",
      "Succeded\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load CMU Pronouncing Dictionary for syllable counting (source: StackOverFlow)\n",
    "    pronouncing_dict = cmudict.dict()\n",
    "\n",
    "\n",
    "    # Function to count syllables in a word using CMU Pronouncing Dictionary\n",
    "    def count_syllables(word):\n",
    "        if word.lower() in pronouncing_dict:\n",
    "            return max([len(list(y for y in x if y[-1].isdigit())) for x in pronouncing_dict[word.lower()]])\n",
    "        # If word not found in CMU Pronouncing Dictionary, return 1 (assume one syllable)\n",
    "        return 1\n",
    "\n",
    "\n",
    "    # Function to calculate Average Number of Words Per Sentence\n",
    "    def calculate_avg_words_per_sentence(text):\n",
    "        sentences = sent_tokenize(text)\n",
    "        total_words = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "        total_sentences = len(sentences)\n",
    "        if total_sentences != 0:\n",
    "            avg_words_per_sentence = total_words / total_sentences\n",
    "        else:\n",
    "            avg_words_per_sentence = 0\n",
    "        return avg_words_per_sentence\n",
    "\n",
    "\n",
    "    # Function to calculate Complex Word Count\n",
    "    def calculate_complex_word_count(text):\n",
    "        words = word_tokenize(text)\n",
    "        complex_words = [word for word in words if len(word) > 6]  # Assume complex words are those with more than 6 characters\n",
    "        return len(complex_words)\n",
    "\n",
    "\n",
    "    # Function to calculate Syllables Per Word\n",
    "    def calculate_syllables_per_word(text):\n",
    "        words = word_tokenize(text)\n",
    "        total_syllables = sum(count_syllables(word) for word in words)\n",
    "        total_words = len(words)\n",
    "        if total_words != 0:\n",
    "            syllables_per_word = total_syllables / total_words\n",
    "        else:\n",
    "            syllables_per_word = 0\n",
    "        return syllables_per_word\n",
    "\n",
    "\n",
    "    # Function to calculate Personal Pronouns\n",
    "    def calculate_personal_pronouns(text):\n",
    "        personal_pronouns = ['I', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves',\n",
    "                             'you', 'your', 'yours', 'yourself', 'yourselves',\n",
    "                             'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself',\n",
    "                             'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves']\n",
    "        words = word_tokenize(text)\n",
    "        personal_pronoun_count = sum(1 for word in words if word.lower() in personal_pronouns)\n",
    "        return personal_pronoun_count\n",
    "\n",
    "\n",
    "    # Function to calculate Average Word Length\n",
    "    def calculate_avg_word_length(text):\n",
    "        words = word_tokenize(text)\n",
    "        total_characters = sum(len(word) for word in words)\n",
    "        total_words = len(words)\n",
    "        if total_words != 0:\n",
    "            avg_word_length = total_characters / total_words\n",
    "        else:\n",
    "            avg_word_length = 0\n",
    "        return avg_word_length\n",
    "\n",
    "\n",
    "    # Function to calculate Average Sentence Length\n",
    "    def calculate_average_sentence_length(text):\n",
    "        sentences = sent_tokenize(text)\n",
    "        total_words = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "        total_sentences = len(sentences)\n",
    "        if total_sentences != 0:\n",
    "            average_sentence_length = total_words / total_sentences\n",
    "        else:\n",
    "            average_sentence_length = 0\n",
    "        return average_sentence_length\n",
    "\n",
    "\n",
    "    # Function to calculate Percentage of Complex Words\n",
    "    def calculate_percentage_complex_words(text):\n",
    "        words = word_tokenize(text)\n",
    "        total_words = len(words)\n",
    "        complex_words = [word for word in words if len(word) > 6]  # Assume complex words are those with more than 6 characters\n",
    "        total_complex_words = len(complex_words)\n",
    "        if total_words != 0:\n",
    "            percentage_complex_words = total_complex_words / total_words * 100\n",
    "        else:\n",
    "            percentage_complex_words = 0\n",
    "        return percentage_complex_words\n",
    "\n",
    "\n",
    "    # Loading Output file:\n",
    "    excel_file = 'Output Data Structure.xlsx'\n",
    "    df = pd.read_excel(excel_file)\n",
    "\n",
    "\n",
    "    # Iterating through each URL_ID\n",
    "    for index, row in df.iterrows():\n",
    "        text_file = row['URL_ID']+\".txt\"\n",
    "        text_filepath = os.path.join('', text_file)\n",
    "        try:\n",
    "            with open(text_filepath, 'r', encoding='latin-1') as file:\n",
    "                text = file.read()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Text file '{text_file}' not found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "\n",
    "            #Start Calculating\n",
    "            avg_words_per_sentence = calculate_avg_words_per_sentence(text)\n",
    "            complex_word_count = calculate_complex_word_count(text)\n",
    "            syllables_per_word = calculate_syllables_per_word(text)\n",
    "            personal_pronouns = calculate_personal_pronouns(text)\n",
    "            avg_word_length = calculate_avg_word_length(text)\n",
    "            average_sentence_length = calculate_average_sentence_length(text)\n",
    "            percentage_complex_words = calculate_percentage_complex_words(text)\n",
    "            fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
    "\n",
    "\n",
    "            # Updating rows\n",
    "            df.at[index, 'AVG NUMBER OF WORDS PER SENTENCE'] = avg_words_per_sentence\n",
    "            df.at[index, 'COMPLEX WORD COUNT'] = complex_word_count\n",
    "            df.at[index, 'SYLLABLE PER WORD'] = syllables_per_word\n",
    "            df.at[index, 'PERSONAL PRONOUNS'] = personal_pronouns\n",
    "            df.at[index, 'AVG WORD LENGTH'] = avg_word_length\n",
    "            df.at[index, 'AVG SENTENCE LENGTH'] = average_sentence_length\n",
    "            df.at[index, 'PERCENTAGE OF COMPLEX WORDS'] = percentage_complex_words\n",
    "            df.at[index, 'FOG INDEX'] = fog_index\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing text file '{text_file}': {str(e)}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "    df.to_excel(excel_file, index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n",
    "else:\n",
    "    print(\"Succeded\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
